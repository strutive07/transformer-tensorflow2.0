{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "transformer_implement_tf2.0",
   "provenance": [],
   "collapsed_sections": [
    "3-AU1l_G5DCy",
    "8G3TYwPj5tSN"
   ],
   "toc_visible": true,
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy9IkwEH5HHB",
    "colab_type": "text"
   },
   "source": [
    "# Transformer tensorflow 2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0fAphmE5RpN",
    "colab_type": "text"
   },
   "source": [
    "## install tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MVxeT3PCPEoM",
    "colab_type": "code",
    "outputId": "b48f1def-623b-4b2d-88bd-c40241775313",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    }
   },
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "# !pip install tensorflow_probability==0.8.0rc0 --upgrade\n",
    "!pip install sentencepiece\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 2.5MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.83\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NI8nUs8nVDBZ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!mkdir -p checkpoints\n",
    "!mkdir -p datasets"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I0-noslgPGMB",
    "colab_type": "code",
    "outputId": "5d6096b8-b481-4448-839f-83b5127f61f1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    }
   },
   "source": [
    "# print tensorflow versions\n",
    "!pip freeze | grep tensorflow\n",
    "!nvidia-smi"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "mesh-tensorflow==0.0.5\n",
      "tensorflow==2.0.0\n",
      "tensorflow-estimator==2.0.0\n",
      "tensorflow-gpu==2.0.0\n",
      "tensorflow-hub==0.6.0\n",
      "tensorflow-metadata==0.14.0\n",
      "tensorflow-probability==0.7.0\n",
      "Fri Oct 18 15:04:51 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   52C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I2KcHsSd-kJZ",
    "colab_type": "code",
    "outputId": "0e2ee78f-99a3-461f-b6c0-0d426c2f8a93",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    }
   },
   "source": [
    "print('is gpu available?: ', tf.test.is_gpu_available())\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "is gpu available?:  True\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 1221983531376434526, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 15163588495924811008\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 6209084037304517343\n",
       " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 11330115994\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 1766124167096464211\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-AU1l_G5DCy",
    "colab_type": "text"
   },
   "source": [
    "## Layer implementation\n",
    "![transformer](https://github.com/strutive07/TIL/raw/master/images/transformer0.PNG)\n",
    "\n",
    "구현할 layer 목록\n",
    "- embedding\n",
    "- positional encoding\n",
    "- encoder\n",
    "- decoder\n",
    "- multi head attention\n",
    "- scaled dot product attention\n",
    "- position wise feed forward network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7diefpC6Bz4",
    "colab_type": "text"
   },
   "source": [
    "### Embedding layer, Positional Encoding\n",
    "word embedding: [tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
    "\n",
    "Convert 2D sequence (batch_size, input_length) -> 3D (batch_size, input_length, $d_{model}$)\n",
    "\n",
    "![positional encoding](https://github.com/strutive07/TIL/raw/master/images/1566655262694.png)\n",
    "\n",
    "Positional encoding 은 $pos/10000^{2i/d_{model}}$ 을 각도로 sin, cos 에 대입했을때 가지는 값이다.\n",
    "짝수는 sin, 홀수는 cos 값을 대입해준다.\n",
    "\n",
    "여기서 pos 는 sequence batch 수, index 는 해당 index 의 embedding dimention 이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fsW9JNVnZEik",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Embeddinglayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        # model hyper parameter variables\n",
    "        super(Embeddinglayer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def call(self, sequences):\n",
    "        max_sequence_len = sequences.shape[1]\n",
    "        output = self.embedding(sequences) * tf.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
    "        output += self.positional_encoding(max_sequence_len)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def positional_encoding(self, max_len):\n",
    "        pos = np.expand_dims(np.arange(0, max_len), axis=1)\n",
    "        index = np.expand_dims(np.arange(0, self.d_model), axis=0)\n",
    "        \n",
    "        pe = self.angle(pos, index)\n",
    "        \n",
    "        pe[:, 0::2] = np.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = np.cos(pe[:, 1::2])        \n",
    "        \n",
    "        pe = np.expand_dims(pe, axis=0)\n",
    "        return tf.cast(pe, dtype=tf.float32)\n",
    "        \n",
    "    def angle(self, pos, index):\n",
    "        return pos / np.power(10000, (index - index % 2) / np.float32(self.d_model))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJCghE8je3kX",
    "colab_type": "text"
   },
   "source": [
    "### Scaled Dot-Product Attention, Multi-Head Attention\n",
    "![대체 텍스트](https://github.com/strutive07/TIL/raw/master/images/1566648835945.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiJVJP60h9hq",
    "colab_type": "text"
   },
   "source": [
    "Scaled Dot-Product Attention 은 learnable parameter 가 없다. 기존 additive attention 은 attention score 를 구하는 구간에 feed forward layer 가 있었지만, scaled dot-product attention 은 dot-product 연산으로 대체하므로 learnable parameter가 존재하지 않는다.\n",
    "\n",
    "따라서 구현도 matmul -> scale -> mask -> softmax -> matmul 순서로 진행해주면된다.\n",
    "주의할점은 추후에 넣어줄 mask 에 encoder 에서도 padding을 학습에 사용하지 않도록 padding mask 를 추가해주어야 한다는것이다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iTuOEEkSZEkj",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_h):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_h = d_h\n",
    "        \n",
    "    def call(self, query, key, value, mask=None):\n",
    "        matmul_q_and_transposed_k = tf.matmul(query, key, transpose_b=True)\n",
    "        scale = tf.sqrt(tf.cast(self.d_h, dtype=tf.float32))\n",
    "        scaled_attention_score = matmul_q_and_transposed_k / scale\n",
    "        if mask is not None:\n",
    "            scaled_attention_score += (mask * -1e9)\n",
    "        \n",
    "        attention_weight = tf.nn.softmax(scaled_attention_score, axis=-1)\n",
    "        \n",
    "        return tf.matmul(attention_weight, value), attention_weight"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RCGL0akh7XG",
    "colab_type": "text"
   },
   "source": [
    "이제 핵심인 multi head attention 을 구현해보자.\n",
    "\n",
    "Multi head attention 은 Query, Key, Value head 수 만큼 나뉜 후, 각각 다른 linear projection, 각각 다른 scaled dot-product 를 진행한 후 concat, linear projection 을 진행하는것으로 논문에 나와있는데, 이는 tf.reshape, tf.transform 으로 한번에 연산할 수 있다.\n",
    "\n",
    "아래 코드의 42번째 줄을 보면, 기존 N 개의 head 로 split 하기 이전, 원본 query, key, value 의 shape 은 (batch_size, seq_len, d_model) 인 것을 알 수 있다. tf.reshape, tf.transform 으로 (batch_size, attention_head_count, seq_len, d_h) 으로 쪼개준다.\n",
    "이는 논문에서 $d_{query} = d_{key} = d_{value} = d_{model}/heads$ 를 구현한것이다.\n",
    "\n",
    "코드를 보면 split 하기 이전에 linear projection 을 진행한것을 볼 수 있는데, 이는 어짜피 reshape 으로 쪼개져서 사용되므로 쪼개고 곱하던, 곱하고 쪼개던 상관이 없다. 따라서 하나의 연산으로 진행해준다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2iASJGPNZEl2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_head_count, d_model, dropout_prob):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # model hyper parameter variables\n",
    "        self.attention_head_count = attention_head_count\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        if d_model % attention_head_count != 0:\n",
    "            raise ValueError(\n",
    "                f\"d_model({d_model}) % attention_head_count({attention_head_count}) is not zero.\"\n",
    "                f\"d_model must be multiple of attention_head_count.\"\n",
    "            )\n",
    "        \n",
    "        self.d_h = d_model // attention_head_count\n",
    "        \n",
    "        self.w_query = tf.keras.layers.Dense(d_model)\n",
    "        self.w_key = tf.keras.layers.Dense(d_model)\n",
    "        self.w_value = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.scaled_dot_product = ScaledDotProductAttention(self.d_h)\n",
    "        \n",
    "        self.ff = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def call(self, query, key, value, mask=None):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        query = self.w_query(query)\n",
    "        key = self.w_key(key)\n",
    "        value = self.w_value(value)\n",
    "        \n",
    "        query = self.split_head(query, batch_size)\n",
    "        key = self.split_head(key, batch_size)\n",
    "        value = self.split_head(value, batch_size)\n",
    "        \n",
    "        output, attention = self.scaled_dot_product(query, key, value, mask)\n",
    "        output = self.concat_head(output, batch_size)\n",
    "        \n",
    "        return self.ff(output), attention\n",
    "        \n",
    "    \n",
    "    def split_head(self, tensor, batch_size):\n",
    "        # input tensor: (batch_size, seq_len, d_model)\n",
    "        return tf.transpose(\n",
    "            tf.reshape(\n",
    "                tensor, \n",
    "                (batch_size, -1, self.attention_head_count, self.d_h)\n",
    "                # tensor: (batch_size, seq_len_splited, attention_head_count, d_h)\n",
    "            ),\n",
    "            [0, 2, 1, 3]\n",
    "            # tensor: (batch_size, attention_head_count, seq_len_splited, d_h)\n",
    "        )\n",
    "    \n",
    "    def concat_head(self, tensor, batch_size):\n",
    "        return tf.reshape(\n",
    "            tf.transpose(tensor, [0, 2, 1, 3]), \n",
    "            (batch_size, -1, self.attention_head_count * self.d_h)\n",
    "        )"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1ou1hSOixMG",
    "colab_type": "text"
   },
   "source": [
    "### Position-Wise Feed-Forward Network\n",
    "![대체 텍스트](https://github.com/strutive07/TIL/raw/master/images/1566658909167.png)\n",
    "\n",
    "Multi head attention 에서 나온 여러가지 attention 정보를 정리해주는 역활인 Position-wise feed-forward network이다.\n",
    "구현은 FF-Relu-Dropout-FF 순서로 sequential 하게 진행하면된다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0d4-dSh5ZEnm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class PositionWiseFeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_point_wise_ff, d_model, dropout_prob):\n",
    "        super(PositionWiseFeedForwardLayer, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_point_wise_ff)\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def call(self, input):\n",
    "        input = self.w_1(input)\n",
    "        input = tf.nn.relu(input)\n",
    "        return self.w_2(input)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDYQgyhXjCZq",
    "colab_type": "text"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "![대체 텍스트](https://github.com/strutive07/TIL/raw/master/images/1566654859388.png)\n",
    "\n",
    "이제 내부 layer 들이 준비되었으니, encoder 를 구현해보자.\n",
    "encoder 는 Multi head attention - Dropout - LayerNorm with Residual connection - Position-wise Feed-Forward - Dropout - LayerNorm with Residual connection 순서로 진행된다.\n",
    "\n",
    "이제 하나하나 layer 를 쌓아보자.\n",
    "\n",
    "특이점은 딱히 없고, tf.add 를 통해 Residual connection 을 잘 해주면될듯하다. 또한 padding 을 학습에 적용하지 못하도록 padding mask 를 잘 넣어주는것도 중요하다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "62uHfgc1ZEpg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "      def __init__(self, attention_head_count, d_model, d_point_wise_ff, dropout_prob):\n",
    "            super(EncoderLayer, self).__init__()\n",
    "\n",
    "            # model hyper parameter variables\n",
    "            self.attention_head_count = attention_head_count\n",
    "            self.d_model = d_model\n",
    "            self.d_point_wise_ff = d_point_wise_ff\n",
    "            self.dropout_prob = dropout_prob\n",
    "\n",
    "            self.multi_head_attention = MultiHeadAttention(attention_head_count, d_model, dropout_prob)\n",
    "            self.dropout_1 = tf.keras.layers.Dropout(dropout_prob)\n",
    "            self.layer_norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "            self.position_wise_feed_forward_layer = PositionWiseFeedForwardLayer(\n",
    "                d_point_wise_ff, \n",
    "                d_model,\n",
    "                dropout_prob\n",
    "            )\n",
    "            self.dropout_2 = tf.keras.layers.Dropout(dropout_prob)\n",
    "            self.layer_norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    \n",
    "      def call(self, input, mask, training):\n",
    "            output, attention = self.multi_head_attention(input, input, input, mask)\n",
    "            output = self.dropout_1(output, training=training)\n",
    "            output = self.layer_norm_1(tf.add(input, output)) # residual network\n",
    "            \n",
    "            output = self.position_wise_feed_forward_layer(output)\n",
    "            output = self.dropout_2(output, training=training)\n",
    "            output = self.layer_norm_2(tf.add(input, output)) #residual network\n",
    "            \n",
    "            return output, attention"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJEcXEKnjgJA",
    "colab_type": "text"
   },
   "source": [
    "### Decoder\n",
    "![대체 텍스트](https://github.com/strutive07/TIL/raw/master/images/1566657684622.png)\n",
    "\n",
    "decoder 는 조금 주의할구간이 있다. 기존 encoder 에서 residual connection을 input 만 진행했다면, 이번에는 decoder input, masked multi-head attention 두게를 해야하므로 변수 사용을 조심하자.\n",
    "\n",
    "또한 encoder 의 output을 key, value 로 사용하고있다. 이번에도 padding mask를 잘 넣어주여야하고, masked multi head attention 에는 look ahead padding 을 넣어주어야한다. 미래의 데이터를 기반으로 현재 데이터를 생성하지 못하도록 마스크를 넣어준다.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "11l-Oq-Mjfg7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_head_count, d_model, d_point_wise_ff, dropout_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # model hyper parameter variables\n",
    "        self.attention_head_count = attention_head_count\n",
    "        self.d_model = d_model\n",
    "        self.d_point_wise_ff = d_point_wise_ff\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        self.masked_multi_head_attention = MultiHeadAttention(attention_head_count, d_model, dropout_prob)\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(dropout_prob)\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.encoder_decoder_attention = MultiHeadAttention(attention_head_count, d_model, dropout_prob)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(dropout_prob)\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.position_wise_feed_forward_layer = PositionWiseFeedForwardLayer(\n",
    "            d_point_wise_ff, \n",
    "            d_model,\n",
    "            dropout_prob\n",
    "        )\n",
    "        self.dropout_3 = tf.keras.layers.Dropout(dropout_prob)\n",
    "        self.layer_norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, decoder_input, encoder_output, look_ahead_mask, padding_mask, training):\n",
    "        output, attention_1 = self.masked_multi_head_attention(\n",
    "            decoder_input,\n",
    "            decoder_input,\n",
    "            decoder_input,\n",
    "            look_ahead_mask\n",
    "        )\n",
    "        output = self.dropout_1(output, training=training)\n",
    "        query = self.layer_norm_1(tf.add(decoder_input, output)) # residual network\n",
    "        output, attention_2 = self.encoder_decoder_attention(\n",
    "            query,\n",
    "            encoder_output,\n",
    "            encoder_output,\n",
    "            padding_mask\n",
    "        )\n",
    "        output = self.dropout_2(output, training=training)\n",
    "        encoder_decoder_attention_output = self.layer_norm_2(tf.add(output, query))\n",
    "        \n",
    "        output = self.position_wise_feed_forward_layer(encoder_decoder_attention_output)\n",
    "        output = self.dropout_3(output, training=training)\n",
    "        output = self.layer_norm_3(tf.add(encoder_decoder_attention_output, output)) #residual network\n",
    "        \n",
    "        return output, attention_1, attention_2"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tulcZr7YLNy",
    "colab_type": "text"
   },
   "source": [
    "### Masking\n",
    "\n",
    "![masking_1](https://user-images.githubusercontent.com/26921984/67160277-331bb100-f38a-11e9-9a4a-c5520fdb1b4a.png)\n",
    "\n",
    "\n",
    "![masking_2](https://user-images.githubusercontent.com/26921984/67160309-7bd36a00-f38a-11e9-991d-5575080a2c3d.png)\n",
    "\n",
    "\n",
    "padding과 decoder의 look a head 부분을 학습하지 못하도록 masking을 해주어야한다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hbcb_38_d4em",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Mask:\n",
    "    @classmethod\n",
    "    def create_padding_mask(cls, sequences):\n",
    "        sequences = tf.cast(tf.math.equal(sequences, 0), dtype=tf.float32)\n",
    "        return sequences[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "    @classmethod\n",
    "    def create_look_ahead_mask(cls, seq_len):\n",
    "        return 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    \n",
    "    @classmethod\n",
    "    def create_masks(cls, input, target):\n",
    "        encoder_padding_mask = Mask.create_padding_mask(input)\n",
    "        \n",
    "        decoder_padding_mask = Mask.create_padding_mask(input)\n",
    "        \n",
    "        look_ahead_mask = tf.maximum(\n",
    "            Mask.create_look_ahead_mask(tf.shape(target)[1]),\n",
    "            Mask.create_padding_mask(target)\n",
    "        )\n",
    "        \n",
    "        return encoder_padding_mask, look_ahead_mask, decoder_padding_mask"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8G3TYwPj5tSN",
    "colab_type": "text"
   },
   "source": [
    "## Make Model\n",
    "\n",
    "자 이제 모든 준비가 끝났다. Encoder 를 잘 스택해서 결과를 얻고, 해당 결과를 스택된 decoder 에 넣어준다.\n",
    "모델의 마지막 부분인 linear projection 을 추가한다.\n",
    "\n",
    "마지막 softmax 는 trainer 에 넣을지 모델에 넣을지 고민중이다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mR1_g_J3ZExE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 input_vocab_size,\n",
    "                 target_vocab_size,\n",
    "                 encoder_count,\n",
    "                 decoder_count,\n",
    "                 attention_head_count,\n",
    "                 d_model,\n",
    "                 d_point_wise_ff,\n",
    "                 dropout_prob):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # model hyper parameter variables\n",
    "        self.encoder_count = encoder_count\n",
    "        self.decoder_count = decoder_count\n",
    "        self.attention_head_count = attention_head_count\n",
    "        self.d_model = d_model,\n",
    "        self.d_point_wise_ff = d_point_wise_ff,\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.encoder_embedding_layer = Embeddinglayer(input_vocab_size, d_model)\n",
    "        self.encoder_embedding_dropout = tf.keras.layers.Dropout(dropout_prob)\n",
    "        self.decoder_embedding_layer = Embeddinglayer(target_vocab_size, d_model)\n",
    "        self.decoder_embedding_dropout = tf.keras.layers.Dropout(dropout_prob)\n",
    "\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(\n",
    "                attention_head_count,\n",
    "                d_model,\n",
    "                d_point_wise_ff,\n",
    "                dropout_prob\n",
    "            ) for _ in range(encoder_count)\n",
    "        ]\n",
    "\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(\n",
    "                attention_head_count,\n",
    "                d_model,\n",
    "                d_point_wise_ff,\n",
    "                dropout_prob\n",
    "            )for _ in range(decoder_count)\n",
    "        ]\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(target_vocab_size)\n",
    "            \n",
    "    def call(self,\n",
    "             input,\n",
    "             target,\n",
    "             input_padding_mask,\n",
    "             look_ahead_mask,\n",
    "             target_padding_mask,\n",
    "             training\n",
    "            ):\n",
    "        encoder_tensor = self.encoder_embedding_layer(input)\n",
    "        encoder_tensor = self.encoder_embedding_dropout(encoder_tensor, training=training)\n",
    "        \n",
    "        for i in range(self.encoder_count):\n",
    "            encoder_tensor, _ = self.encoder_layers[i](encoder_tensor, input_padding_mask, training=training)\n",
    "        target = self.decoder_embedding_layer(target)\n",
    "        decoder_tensor = self.decoder_embedding_dropout(target, training=training)\n",
    "        for i in range(self.decoder_count):\n",
    "            decoder_tensor, _, _ = self.decoder_layers[i](\n",
    "                decoder_tensor,\n",
    "                encoder_tensor,\n",
    "                look_ahead_mask,\n",
    "                target_padding_mask,\n",
    "                training=training\n",
    "            )\n",
    "        return self.linear(decoder_tensor)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhQKU1fhFAld",
    "colab_type": "text"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78QUkA_NVXsq",
    "colab_type": "text"
   },
   "source": [
    "Data는 WMT14 데이터를 사용한다.\n",
    "전처리는 byte pair encoding을 사용한다. Byte pair encoding을 사용하기위해 sentencepiece module을 사용한다.\n",
    "\n",
    "전처리 순서\n",
    "1. Data download\n",
    "2. Data validation, 개행 단위 문장 분리\n",
    "3. Training byte pair encoding \n",
    "4. Encoding data with trained byte pair encoding\n",
    "5. Split training set, validation set\n",
    "6. Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JTeO32-QMWv3",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sentencepiece\n",
    "\n",
    "class DataLoader:\n",
    "    DIR = None\n",
    "    PATHS = {}\n",
    "    BPE_VOCAB_SIZE=0\n",
    "    dictionary = {\n",
    "        'source': {\n",
    "            'token2idx':None,\n",
    "            'idx2token':None,\n",
    "        },\n",
    "        'target': {\n",
    "            'token2idx':None,\n",
    "            'idx2token':None,\n",
    "        }\n",
    "    }\n",
    "    CONFIG = {\n",
    "        'wmt14/en-de': {\n",
    "            'source_lang': 'en',\n",
    "            'target_lang': 'de',\n",
    "            'base_url': 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/',\n",
    "            'train_files': ['train.en', 'train.de'],\n",
    "            'vocab_files': ['vocab.50K.en', 'vocab.50K.de'],\n",
    "            'dictionary_files': ['dict.en-de'],\n",
    "            'test_files': [\n",
    "                'newstest2012.en', 'newstest2012.de',\n",
    "                'newstest2013.en', 'newstest2013.de',\n",
    "                'newstest2014.en', 'newstest2014.de',\n",
    "                'newstest2015.en', 'newstest2015.de',\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    BPE_MODEL_SUFFIX= '.model'\n",
    "    BPE_VOCAB_SUFFIX= '.vocab'\n",
    "    BPE_RESULT_SUFFIX= '.sequences'\n",
    "    SEQ_MAX_LEN = {\n",
    "        'source': 100,\n",
    "        'target': 100\n",
    "    }\n",
    "    DATA_LIMIT = None\n",
    "    TRAIN_RATIO = 0.9\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "    source_sp = None\n",
    "    target_sp = None\n",
    "\n",
    "    def __init__(self, dataset_name, data_dir, batch_size=16, bpe_vocab_size=32000, seq_max_len_source=100, seq_max_len_target=100, data_limit=None, train_ratio=0.9):\n",
    "        if dataset_name is None or data_dir is None:\n",
    "            raise ValueError('dataset_name and data_dir must be defined')\n",
    "        self.DIR = data_dir\n",
    "        self.DATASET = dataset_name\n",
    "        self.BPE_VOCAB_SIZE = bpe_vocab_size\n",
    "        self.SEQ_MAX_LEN['source'] = seq_max_len_source\n",
    "        self.SEQ_MAX_LEN['target'] = seq_max_len_target\n",
    "        self.DATA_LIMIT = data_limit\n",
    "        self.TRAIN_RATIO = train_ratio\n",
    "        self.BATCH_SIZE = batch_size\n",
    "\n",
    "        self.PATHS['source_data'] = os.path.join(self.DIR, self.CONFIG[self.DATASET]['train_files'][0])\n",
    "        self.PATHS['source_bpe_prefix'] = self.PATHS['source_data'] + '.segmented'\n",
    "\n",
    "        self.PATHS['target_data'] = os.path.join(self.DIR, self.CONFIG[self.DATASET]['train_files'][1])\n",
    "        self.PATHS['target_bpe_prefix'] = self.PATHS['target_data'] + '.segmented'\n",
    "\n",
    "    def load(self):\n",
    "        print('#1 download data')\n",
    "        self.download_dataset()\n",
    "\n",
    "        print('#2 parse data')\n",
    "        source_data = self.parse_data_and_save(self.PATHS['source_data'])\n",
    "        target_data = self.parse_data_and_save(self.PATHS['target_data'])\n",
    "        \n",
    "        print('#3 train bpe')\n",
    "        \n",
    "        self.train_bpe(self.PATHS['source_data'], self.PATHS['source_bpe_prefix'])\n",
    "        self.train_bpe(self.PATHS['target_data'], self.PATHS['target_bpe_prefix'])\n",
    "\n",
    "        print('#4 load bpe vocab')\n",
    "        self.load_bpe_encoder()\n",
    "\n",
    "        print('#5 encode data with bpe')\n",
    "        source_sequences = self.texts_to_sequences(\n",
    "            self.bulk_sentence_piece(\n",
    "                source_data,\n",
    "                self.PATHS['source_bpe_prefix'] + self.BPE_MODEL_SUFFIX,\n",
    "                self.PATHS['source_bpe_prefix'] + self.BPE_RESULT_SUFFIX\n",
    "            ),\n",
    "            mode=\"source\"\n",
    "        )\n",
    "        target_sequences = self.texts_to_sequences(\n",
    "            self.bulk_sentence_piece(\n",
    "                target_data,\n",
    "                self.PATHS['target_bpe_prefix'] + self.BPE_MODEL_SUFFIX,\n",
    "                self.PATHS['target_bpe_prefix'] + self.BPE_RESULT_SUFFIX\n",
    "            ),\n",
    "            mode=\"target\"\n",
    "        )\n",
    "\n",
    "        print('source sequence example:', source_sequences[0])\n",
    "        print('target sequence example:', target_sequences[0])\n",
    "\n",
    "\n",
    "        source_sequences_train, source_sequences_val, target_sequences_train, target_sequences_val = train_test_split(\n",
    "            source_sequences, target_sequences, train_size=self.TRAIN_RATIO\n",
    "        )\n",
    "\n",
    "        if self.DATA_LIMIT is not None:\n",
    "            print('data size limit ON. limit size:', DATA_LIMIT)\n",
    "            source_sequences_train = source_sequences_train[:DATA_LIMIT]\n",
    "            target_sequences_train = target_sequences_train[:DATA_LIMIT]\n",
    "\n",
    "        print('source_sequences_train', len(source_sequences_train))\n",
    "        print('source_sequences_val', len(source_sequences_val))\n",
    "        print('target_sequences_train', len(target_sequences_train))\n",
    "        print('target_sequences_val', len(target_sequences_val))\n",
    "\n",
    "        print('train set size: ', len(source_sequences_train))\n",
    "        print('validation set size: ', len(source_sequences_val))\n",
    "        TRAIN_SET_SIZE = len(source_sequences_train)\n",
    "        VALIDATION_SET_SIZE = len(source_sequences_val)\n",
    "\n",
    "        train_dataset = self.create_dataset(\n",
    "            source_sequences_train,\n",
    "            target_sequences_train\n",
    "        )\n",
    "\n",
    "        val_dataset = self.create_dataset(\n",
    "            source_sequences_val,\n",
    "            target_sequences_val\n",
    "        )\n",
    "\n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def load_test(self, index=0):\n",
    "        if index < 0 or index >= len(self.CONFIG[self.DATASET]['test_files']) // 2:\n",
    "            raise ValueError('test file index out of range. min: 0, max: {}'.format(len(self.CONFIG[self.DATASET]['test_files']) // 2 - 1))\n",
    "        print('#1 download data')\n",
    "        self.download_dataset()\n",
    "\n",
    "        print('#2 parse data')\n",
    "\n",
    "        source_test_data_path = os.path.join(self.DIR, self.CONFIG[self.DATASET]['test_files'][index * 2])\n",
    "        target_test_data_path = os.path.join(self.DIR, self.CONFIG[self.DATASET]['test_files'][index * 2 + 1])\n",
    "\n",
    "        source_data = self.parse_data_and_save(source_test_data_path)\n",
    "        target_data = self.parse_data_and_save(target_test_data_path)\n",
    "\n",
    "        print('#3 load bpe vocab')\n",
    "\n",
    "        self.dictionary['source']['token2idx'], self.dictionary['source']['idx2token'] = self.load_bpe_vocab(\n",
    "            self.PATHS['source_bpe_prefix'] + self.BPE_VOCAB_SUFFIX)\n",
    "        self.dictionary['target']['token2idx'], self.dictionary['target']['idx2token'] = self.load_bpe_vocab(\n",
    "            self.PATHS['target_bpe_prefix'] + self.BPE_VOCAB_SUFFIX)\n",
    "\n",
    "        return source_data, target_data\n",
    "    \n",
    "    def load_bpe_encoder(self):\n",
    "        self.dictionary['source']['token2idx'], self.dictionary['source']['idx2token'] =  self.load_bpe_vocab(self.PATHS['source_bpe_prefix'] + self.BPE_VOCAB_SUFFIX)\n",
    "        self.dictionary['target']['token2idx'], self.dictionary['target']['idx2token'] =  self.load_bpe_vocab(self.PATHS['target_bpe_prefix'] + self.BPE_VOCAB_SUFFIX)\n",
    "\n",
    "    def download_dataset(self):\n",
    "        for file in (self.CONFIG[self.DATASET]['train_files']\n",
    "                     + self.CONFIG[self.DATASET]['vocab_files']\n",
    "                     + self.CONFIG[self.DATASET]['dictionary_files']\n",
    "                     + self.CONFIG[self.DATASET]['test_files']):\n",
    "            self._download(\"{}{}\".format(self.CONFIG[self.DATASET]['base_url'], file))\n",
    "\n",
    "    def _download(self, url):\n",
    "        path = os.path.join(self.DIR, url.split('/')[-1])\n",
    "        if not os.path.exists(path):\n",
    "            with TqdmCustom(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=url) as t:\n",
    "                urlretrieve(url, path, t.update_to)\n",
    "    \n",
    "    def parse_data_and_save(self, path):\n",
    "        print('load data from {}'.format(path))\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            lines = f.read().strip().split('\\n')\n",
    "\n",
    "        if lines is None:\n",
    "            raise ValueError('Vocab file is invalid')\n",
    "            \n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(lines))\n",
    "        \n",
    "        return lines\n",
    "    \n",
    "    def train_bpe(self, data_path, model_prefix):\n",
    "        model_path = model_prefix + self.BPE_MODEL_SUFFIX\n",
    "        vocab_path = model_prefix + self.BPE_VOCAB_SUFFIX\n",
    "\n",
    "        if not(os.path.exists(model_path) and os.path.exists(vocab_path)):\n",
    "            print('bpe model does not exist. train bpe. model path:', model_path, ' vocab path:', vocab_path)\n",
    "            train_source_params = \"--input={} \\\n",
    "                --pad_id=0 \\\n",
    "                --unk_id=1 \\\n",
    "                --bos_id=2 \\\n",
    "                --eos_id=3 \\\n",
    "                --model_prefix={} \\\n",
    "                --vocab_size={} \\\n",
    "                --model_type=bpe \".format(\n",
    "                data_path,\n",
    "                model_prefix,\n",
    "                self.BPE_VOCAB_SIZE\n",
    "            )\n",
    "            sentencepiece.SentencePieceTrainer.Train(train_source_params)\n",
    "        else:\n",
    "            print('bpe model exist. load bpe. model path:', model_path, ' vocab path:', vocab_path)\n",
    "\n",
    "    def bulk_sentence_piece(self, source_data, source_bpe_model_path, result_data_path):\n",
    "        if os.path.exists(result_data_path):\n",
    "            print('encoded data exist. load data. path:', result_data_path)\n",
    "            with open(result_data_path, 'r', encoding='utf-8') as f:\n",
    "                 sequences = f.read().strip().split('\\n')\n",
    "                 return sequences\n",
    "\n",
    "        sp = sentencepiece.SentencePieceProcessor()\n",
    "        sp.load(source_bpe_model_path)\n",
    "        print('encoded data does not exist. encode data. path:', result_data_path)\n",
    "        sequences = []\n",
    "        with open(result_data_path, 'w') as f:\n",
    "            for sentence in tqdm(source_data):\n",
    "                pieces = sp.EncodeAsPieces(sentence)\n",
    "                sequence = \" \".join(pieces)\n",
    "                sequences.append(sequence)\n",
    "                f.write(sequence + \"\\n\")\n",
    "        return sequences\n",
    "    \n",
    "    def encode_data(self, input, mode='source'):\n",
    "        if mode != 'source' and mode != 'target':\n",
    "            ValueError('not allowed mode.')\n",
    "\n",
    "        if mode == 'source':\n",
    "            if self.source_sp is None:\n",
    "                self.source_sp = sentencepiece.SentencePieceProcessor()\n",
    "                self.source_sp.load(self.PATHS['source_bpe_prefix'] + self.BPE_MODEL_SUFFIX)\n",
    "\n",
    "            pieces = self.source_sp.EncodeAsPieces(input)\n",
    "            sequence = \" \".join(pieces)\n",
    "\n",
    "            return sequence\n",
    "        elif mode == 'target':\n",
    "            if self.target_sp is None:\n",
    "                self.target_sp = sentencepiece.SentencePieceProcessor()\n",
    "                self.target_sp.load(self.PATHS['target_bpe_prefix'] + self.BPE_MODEL_SUFFIX)\n",
    "            \n",
    "            pieces = self.target_sp.EncodeAsPieces(input)\n",
    "            sequence = \" \".join(pieces)\n",
    "\n",
    "            return sequence\n",
    "        else:\n",
    "            ValueError('not allowed mode.')\n",
    "    \n",
    "    def load_bpe_vocab(self, bpe_vocab_path):\n",
    "        vocab = [line.split()[0] for line in open(bpe_vocab_path, 'r').read().splitlines()]\n",
    "        token2idx = {}\n",
    "        idx2token = {}\n",
    "\n",
    "        for idx, token in enumerate(vocab):\n",
    "            token2idx[token] = idx\n",
    "            idx2token[idx] = token\n",
    "        return token2idx, idx2token\n",
    "    \n",
    "    def texts_to_sequences(self, texts, mode='source'):\n",
    "        if mode != 'source' and mode != 'target':\n",
    "            ValueError('not allowed mode.')\n",
    "            \n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            text_list = [\"<s>\"] + text.split() + [\"</s>\"]\n",
    "            \n",
    "            sequence = [\n",
    "                        self.dictionary[mode]['token2idx'].get(\n",
    "                            token, self.dictionary[mode]['token2idx'][\"<unk>\"]\n",
    "                        )\n",
    "                        for token in text_list\n",
    "            ]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences, mode='source'):\n",
    "        if mode != 'source' and mode != 'target':\n",
    "            ValueError('not allowed mode.')\n",
    "            \n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            if mode == 'source':\n",
    "                if self.source_sp is None:\n",
    "                    self.source_sp = sentencepiece.SentencePieceProcessor()\n",
    "                    self.source_sp.load(self.PATHS['source_bpe_prefix'] + self.BPE_MODEL_SUFFIX)\n",
    "                text = self.source_sp.DecodeIds(sequence)\n",
    "            else:\n",
    "                if self.target_sp is None:\n",
    "                    self.target_sp = sentencepiece.SentencePieceProcessor()\n",
    "                    self.target_sp.load(self.PATHS['target_bpe_prefix'] + self.BPE_MODEL_SUFFIX)\n",
    "                text = self.target_sp.DecodeIds(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def create_dataset(self, source_sequences, target_sequences):\n",
    "        new_source_sequences = []\n",
    "        new_target_sequences = []\n",
    "        for source, target in zip(source_sequences, target_sequences):\n",
    "            if len(source) > self.SEQ_MAX_LEN['source']:\n",
    "                continue\n",
    "            if len(target) > self.SEQ_MAX_LEN['target']:\n",
    "                continue\n",
    "            new_source_sequences.append(source)\n",
    "            new_target_sequences.append(target)\n",
    "            \n",
    "        source_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences=source_sequences, maxlen=self.SEQ_MAX_LEN['source'], padding='post'\n",
    "        )\n",
    "        target_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences=target_sequences, maxlen=self.SEQ_MAX_LEN['target'], padding='post'\n",
    "        )\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (source_sequences, target_sequences)\n",
    "        )\n",
    "        dataset = dataset.padded_batch(self.BATCH_SIZE, self.DATA_SHAPES, self.PADDINGS)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class TqdmCustom(tqdm):\n",
    "\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7rUFGpdVHWW",
    "colab_type": "text"
   },
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puRcoOcFu8tp",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_uBurIB3AxDz",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# hyper paramaters\n",
    "TRAIN_RATIO = 0.9\n",
    "D_POINT_WISE_FF = 2048\n",
    "D_MODEL = 512\n",
    "ENCODER_COUNT = DECODER_COUNT = 6\n",
    "EPOCHS = 20\n",
    "ATTENTION_HEAD_COUNT = 8\n",
    "DROPOUT_PROB = 0.1\n",
    "BATCH_SIZE = 16\n",
    "SEQ_MAX_LEN_SOURCE = 100\n",
    "SEQ_MAX_LEN_TARGET = 100\n",
    "BPE_VOCAB_SIZE = 32000\n",
    "DATA_LIMIT = None"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv94mbQWUH4C",
    "colab_type": "text"
   },
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoYXid7wUK25",
    "colab_type": "text"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D3AMejqJInJu",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data_loader = DataLoader(\n",
    "    'wmt14/en-de', \n",
    "    'datasets',\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UBN1N7kqNN4N",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "dataset, val_dataset = data_loader.load()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi44qjryUQEC",
    "colab_type": "text"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lbZlKgePJ4V",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KAaqocWSPR3E",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    " transformer = Transformer(\n",
    "     input_vocab_size=BPE_VOCAB_SIZE,\n",
    "     target_vocab_size=BPE_VOCAB_SIZE,\n",
    "     encoder_count=ENCODER_COUNT,\n",
    "     decoder_count=DECODER_COUNT,\n",
    "     attention_head_count=ATTENTION_HEAD_COUNT,\n",
    "     d_model=D_MODEL,\n",
    "     d_point_wise_ff=D_POINT_WISE_FF,\n",
    "     dropout_prob=DROPOUT_PROB\n",
    " )"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyYMsTFAWyuM",
    "colab_type": "text"
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "![optimizer](https://user-images.githubusercontent.com/26921984/67160202-4c702d80-f389-11e9-9ec3-6d43e222ff6f.png)\n",
    "\n",
    "Adam optimizer를 사용하고, optimizer의 learning rate에 warmup 을 넣어주기위해 tf.keras.optimizers.schedules.LearningRateSchedule 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t53ufWmcTETR",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        \n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fuDA3E_ySsjp",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer = tf.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "loss_object = tf.losses.CategoricalCrossentropy(from_logits=True, reduction='none')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMe6ACkBZpyY",
    "colab_type": "text"
   },
   "source": [
    "### Label smoothing\n",
    "\n",
    "![label_smoothing](https://user-images.githubusercontent.com/26921984/67160345-b806ca80-f38a-11e9-8035-019f8542fdf4.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MJswQD45tqa-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def label_smoothing(target_data, depth, epsilon=0.1):\n",
    "    target_data_one_hot = tf.one_hot(target_data, depth=depth)\n",
    "    n = target_data_one_hot.get_shape().as_list()[-1]\n",
    "    return ((1 - epsilon) * target_data_one_hot) + (epsilon / n)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p8EHzqc0TCeV",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import time\n",
    "import datetime\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            dataset,\n",
    "            loss_object=None,\n",
    "            optimizer=None,\n",
    "            checkpoint_dir='./checkpoints',\n",
    "            batch_size=None,\n",
    "            distribute_strategy=None,\n",
    "            vocab_size=32000,\n",
    "            epoch=20,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.distribute_strategy = distribute_strategy\n",
    "        self.model = model\n",
    "        self.loss_object = loss_object\n",
    "        self.optimizer = optimizer\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.vocab_size = vocab_size\n",
    "        self.epoch = epoch\n",
    "        self.dataset = dataset\n",
    "\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        if self.optimizer is None:\n",
    "            self.checkpoint = tf.train.Checkpoint(step=tf.Variable(1), model=self.model)\n",
    "        else:\n",
    "            self.checkpoint = tf.train.Checkpoint(step=tf.Variable(1), optimizer=self.optimizer, model=self.model)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, self.checkpoint_dir, max_to_keep=3)\n",
    "\n",
    "        # metrics\n",
    "        self.train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "        self.validation_loss = tf.keras.metrics.Mean('validation_loss', dtype=tf.float32)\n",
    "        self.validation_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('validation_accuracy')\n",
    "\n",
    "    def multi_gpu_train(self, reset_checkpoint=False):\n",
    "        with self.distribute_strategy.scope():\n",
    "            self.dataset = self.distribute_strategy.experimental_distribute_dataset(self.dataset)\n",
    "            self.trainer(reset_checkpoint=reset_checkpoint, is_distributed=True)\n",
    "\n",
    "    \n",
    "    def single_gpu_train(self, reset_checkpoint=False):\n",
    "        self.trainer(reset_checkpoint=reset_checkpoint, is_distributed=False)\n",
    "\n",
    "    def trainer(self, reset_checkpoint, is_distributed=False):\n",
    "        current_day = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "        train_log_dir = './logs/gradient_tape/' + current_day + '/train'\n",
    "        os.makedirs(train_log_dir, exist_ok=True)\n",
    "        train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "        if not reset_checkpoint:\n",
    "            if self.checkpoint_manager.latest_checkpoint:\n",
    "                print(\"Restored from {}\".format(self.checkpoint_manager.latest_checkpoint))\n",
    "            else:\n",
    "                print(\"Initializing from scratch.\")\n",
    "\n",
    "            self.checkpoint.restore(\n",
    "                self.checkpoint_manager.latest_checkpoint\n",
    "            )\n",
    "        else:\n",
    "            print(\"reset and initializing from scratch.\")\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            start = time.time()\n",
    "            print('start learning')\n",
    "\n",
    "            for (batch, (input, target)) in enumerate(self.dataset):\n",
    "                if is_distributed:\n",
    "                    loss = self.distributed_train_step(input, target)\n",
    "                else:\n",
    "                    loss = self.train_step(input, target)\n",
    "\n",
    "                self.checkpoint.step.assign_add(1)\n",
    "                if batch % 50 == 0:\n",
    "                    print(\n",
    "                        \"Epoch: {}, Batch: {}, Loss:{}, Accuracy: {}\".format(epoch, batch, self.train_loss.result(),\n",
    "                                                                             self.train_accuracy.result()))\n",
    "                if batch % 10000 == 0 and batch != 0:\n",
    "                    tf.py_function(self.checkpoint_manager.save, [], [])\n",
    "            print(\"{} | Epoch: {} Loss:{}, Accuracy: {}, time: {} sec\".format(\n",
    "                datetime.datetime.now(), epoch, self.train_loss.result(), self.train_accuracy.result(),\n",
    "                time.time() - start\n",
    "            ))\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('train_loss', self.train_loss.result(), step=epoch)\n",
    "                tf.summary.scalar('train_accuracy', self.train_accuracy.result(), step=epoch)\n",
    "\n",
    "            tf.py_function(self.checkpoint_manager.save, [], [])\n",
    "\n",
    "            self.train_loss.reset_states()\n",
    "            self.train_accuracy.reset_states()\n",
    "            self.validation_loss.reset_states()\n",
    "            self.validation_accuracy.reset_states()\n",
    "\n",
    "        tf.py_function(self.checkpoint_manager.save, [], [])\n",
    "\n",
    "    def train_step(self, input, target):\n",
    "        target_include_start = target[:, :-1]\n",
    "        target_include_end = target[:, 1:]\n",
    "        encoder_padding_mask, look_ahead_mask, decoder_padding_mask = Mask.create_masks(\n",
    "            input, target_include_start\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.model.call(\n",
    "                input=input,\n",
    "                target=target_include_start,\n",
    "                input_padding_mask=encoder_padding_mask,\n",
    "                look_ahead_mask=look_ahead_mask,\n",
    "                target_padding_mask=decoder_padding_mask,\n",
    "                training=True\n",
    "            )\n",
    "\n",
    "            loss = self.loss_function(target_include_end, pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(target_include_end, pred)\n",
    "        if self.distribute_strategy is None:\n",
    "            return tf.reduce_mean(loss)\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    def loss_function(self, real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        real_one_hot = label_smoothing(real, depth=self.vocab_size)\n",
    "        loss = self.loss_object(real_one_hot, pred)\n",
    "\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    @tf.function\n",
    "    def distributed_train_step(self, input, target):\n",
    "        loss = self.distribute_strategy.experimental_run_v2(self.train_step, args=(input, target))\n",
    "        loss_value = self.distribute_strategy.reduce(tf.distribute.ReduceOp.MEAN, loss, axis=None)\n",
    "        return tf.reduce_mean(loss_value)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCzSDqskUV_x",
    "colab_type": "text"
   },
   "source": [
    "### Single GPU learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "POgCdnRrnr70",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "trainer = Trainer(\n",
    "    model=transformer,\n",
    "    dataset=dataset,\n",
    "    loss_object=loss_object,\n",
    "    optimizer=optimizer,\n",
    "    checkpoint_dir='./checkpoints2',\n",
    "    vocab_size=BPE_VOCAB_SIZE,\n",
    "    epoch=EPOCHS,\n",
    ")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LdZGtT-Vke84",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "trainer.single_gpu_train(True)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB0iORxuUbkR",
    "colab_type": "text"
   },
   "source": [
    "### Multi GPU Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tZeh7faUZnv",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JLgmMexLFiH7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "trainer = Trainer(\n",
    "    model=transformer,\n",
    "    dataset=dataset,\n",
    "    loss_object=loss_object,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    distribute_strategy=strategy,\n",
    "    vocab_size=BPE_VOCAB_SIZE,\n",
    "    epoch=EPOCHS,\n",
    ")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OINZ7vt0TYZY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "trainer.multi_gpu_train(True)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX86MR_mUe97",
    "colab_type": "text"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RxJ-WvkVn_Ly",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def translate(input, data_loader, trainer, seq_max_len_target=100):\n",
    "    if data_loader is None:\n",
    "        ValueError('data loader is None')\n",
    "    \n",
    "    if trainer is None:\n",
    "        ValueError('trainer is None')\n",
    "\n",
    "    if trainer.model is None:\n",
    "        ValueError('model is None')\n",
    "    \n",
    "    if not isinstance(seq_max_len_target, int):\n",
    "        ValueError('seq_max_len_target is not int')\n",
    "\n",
    "    encoded_data = data_loader.encode_data(input, mode='source')\n",
    "    encoded_data = data_loader.texts_to_sequences([encoded_data])\n",
    "    encoder_input = tf.convert_to_tensor(\n",
    "        encoded_data,\n",
    "        dtype=tf.int32\n",
    "    )\n",
    "    decoder_input = [data_loader.dictionary['target']['token2idx']['<s>']]\n",
    "    decoder_input = tf.expand_dims(decoder_input, 0)\n",
    "    decoder_end_token = data_loader.dictionary['target']['token2idx']['</s>']\n",
    "    \n",
    "    for i in range(SEQ_MAX_LEN_TARGET):\n",
    "        encoder_padding_mask, look_ahead_mask, decoder_padding_mask = Mask.create_masks(\n",
    "            encoder_input, decoder_input\n",
    "        )\n",
    "        pred = trainer.model.call(\n",
    "            input=encoder_input,\n",
    "            target=decoder_input,\n",
    "            input_padding_mask=encoder_padding_mask,\n",
    "            look_ahead_mask=look_ahead_mask,\n",
    "            target_padding_mask=decoder_padding_mask,\n",
    "            training=False\n",
    "        )\n",
    "        pred = pred[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(pred, axis=-1), dtype=tf.int32)\n",
    "        \n",
    "        if predicted_id == decoder_end_token:\n",
    "            return tf.squeeze(decoder_input, axis=0)\n",
    "        decoder_input = tf.concat([decoder_input, predicted_id], axis=-1)\n",
    "\n",
    "    \n",
    "    return tf.squeeze(decoder_input, axis=0)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2X4hu4t0UqIg",
    "colab_type": "text"
   },
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YruJgoYwTh3e",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data_loader.load_bpe_encoder()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nkQEuKoLcOyj",
    "colab_type": "code",
    "outputId": "081865d8-e90b-49a3-ea51-03e715f9beca",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    }
   },
   "source": [
    "import time\n",
    "import datetime\n",
    "from google.colab import files\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=transformer,\n",
    "    dataset=None,\n",
    "    loss_object=None,\n",
    "    optimizer=None,\n",
    "    checkpoint_dir='./checkpoints'\n",
    ")\n",
    "if trainer.checkpoint_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(trainer.checkpoint_manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "trainer.checkpoint.restore(\n",
    "    trainer.checkpoint_manager.latest_checkpoint\n",
    ")"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Restored from ./checkpoints/ckpt-92\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f882013de80>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 32
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AbR6cRr8siaE",
    "colab_type": "code",
    "outputId": "edc98c93-1a93-44cc-ab27-2e6271080e49",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "output = translate('I love apple.', data_loader, trainer)\n",
    "res = data_loader.sequences_to_texts([output.numpy().tolist()], mode='target')\n",
    "print(res)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "['Ich liebe Apple .']\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqbcrhWPUsKR",
    "colab_type": "text"
   },
   "source": [
    "### Calculate BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wzgdcL51Kjou",
    "colab_type": "code",
    "outputId": "7211f917-11c5-4a9a-92b5-73a710b4a9ee",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    }
   },
   "source": [
    "source_data, target_data = data_loader.load_test(index=3)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "#1 download data\n",
      "#2 parse data\n",
      "load data from datasets/newstest2015.en\n",
      "load data from datasets/newstest2015.de\n",
      "#3 load bpe vocab\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EpmNWqlyau6H",
    "colab_type": "code",
    "outputId": "7e03437b-1a7c-41c6-9969-e4ff8ea1ae78",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "translated_data = []\n",
    "data = zip(source_data, target_data)\n",
    "\n",
    "for source, target in tqdm(data):\n",
    "    output = translate(source, data_loader, trainer)\n",
    "    res = data_loader.sequences_to_texts([output.numpy().tolist()], mode='target')\n",
    "    translated_data.append({\n",
    "        'source': source,\n",
    "        'target': target,\n",
    "        'output': res\n",
    "    })"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2169it [3:40:42,  5.42s/it]\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ped1hlftMH3Z",
    "colab_type": "code",
    "outputId": "331c4ee4-c445-46a1-f543-96ef6651877f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "import pickle\n",
    "with open('translated_data_2015_with_labelsmoothing_checkpoint_92.pickle', 'wb') as f:\n",
    "    pickle.dump(translated_data, f)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Copying file://translated_data_2015_with_labelsmoothing_checkpoint_92.pickle [Content-Type=application/octet-stream]...\n",
      "-\n",
      "Operation completed over 1 objects/908.5 KiB.                                    \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vsTpum6gWaIv",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import pickle\n",
    "with open('translated_data_2014.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GxtmNUqKWiW2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "translated = \"\"\n",
    "for d in data:\n",
    "    translated += d['output'][0] + '\\n'\n",
    "with open('datasets/res', 'w') as f:\n",
    "    f.write(translated)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7eNhjMaWVk6f",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os, re\n",
    "ref = 'datasets/newstest2014.de'\n",
    "translation = 'datasets/res'\n",
    "get_bleu_score = \"perl datasets/multi-bleu.perl {} < {} > {}\".format(ref, translation, \"temp\")\n",
    "os.system(get_bleu_score)\n",
    "bleu_score_report = open(\"temp\", \"r\").read()\n",
    "with open(translation, \"a\") as fout:\n",
    "    fout.write(\"\\n{}\".format(bleu_score_report))\n",
    "score = re.findall(\"BLEU = ([^,]+)\", bleu_score_report)[0]\n",
    "new_translation = translation + \"B{}\".format(score)\n",
    "os.system(\"mv {} {}\".format(translation, new_translation))\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pm7KrRwdXbaN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "bleu_score_report"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}